<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Taylor-Series based pruning criteria for weight pruning in neural networks | Freddy&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="Summary This article discusses how the Taylor series motivates two commonly used pruning criteria for neural networks, namely
Magnitude based pruning Fisher information based pruning The motivation for this blogpost is to highlight that those pruning criteria are closely related while having different bounds on the resulting errors after pruning. I created this blogpost because I found this information lacking in many papers that used the concepts introduced below.
Introduction A common fully-connected neural network - as the name suggests - possesses &ldquo;synapses&rdquo;, i.">
<meta name="author" content="Frederic Mrozinski">
<link rel="canonical" href="https://fredericmrozinski.github.io/built-blog/posts/neuron-pruning-1/">
<link crossorigin="anonymous" href="/built-blog/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://fredericmrozinski.github.io/built-blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://fredericmrozinski.github.io/built-blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://fredericmrozinski.github.io/built-blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://fredericmrozinski.github.io/built-blog/apple-touch-icon.png">
<link rel="mask-icon" href="https://fredericmrozinski.github.io/built-blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://fredericmrozinski.github.io/built-blog/posts/neuron-pruning-1/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  

<meta property="og:title" content="Taylor-Series based pruning criteria for weight pruning in neural networks" />
<meta property="og:description" content="Summary This article discusses how the Taylor series motivates two commonly used pruning criteria for neural networks, namely
Magnitude based pruning Fisher information based pruning The motivation for this blogpost is to highlight that those pruning criteria are closely related while having different bounds on the resulting errors after pruning. I created this blogpost because I found this information lacking in many papers that used the concepts introduced below.
Introduction A common fully-connected neural network - as the name suggests - possesses &ldquo;synapses&rdquo;, i." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://fredericmrozinski.github.io/built-blog/posts/neuron-pruning-1/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-07-22T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-07-22T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Taylor-Series based pruning criteria for weight pruning in neural networks"/>
<meta name="twitter:description" content="Summary This article discusses how the Taylor series motivates two commonly used pruning criteria for neural networks, namely
Magnitude based pruning Fisher information based pruning The motivation for this blogpost is to highlight that those pruning criteria are closely related while having different bounds on the resulting errors after pruning. I created this blogpost because I found this information lacking in many papers that used the concepts introduced below.
Introduction A common fully-connected neural network - as the name suggests - possesses &ldquo;synapses&rdquo;, i."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://fredericmrozinski.github.io/built-blog/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Taylor-Series based pruning criteria for weight pruning in neural networks",
      "item": "https://fredericmrozinski.github.io/built-blog/posts/neuron-pruning-1/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Taylor-Series based pruning criteria for weight pruning in neural networks",
  "name": "Taylor-Series based pruning criteria for weight pruning in neural networks",
  "description": "Summary This article discusses how the Taylor series motivates two commonly used pruning criteria for neural networks, namely\nMagnitude based pruning Fisher information based pruning The motivation for this blogpost is to highlight that those pruning criteria are closely related while having different bounds on the resulting errors after pruning. I created this blogpost because I found this information lacking in many papers that used the concepts introduced below.\nIntroduction A common fully-connected neural network - as the name suggests - possesses \u0026ldquo;synapses\u0026rdquo;, i.",
  "keywords": [
    
  ],
  "articleBody": "Summary This article discusses how the Taylor series motivates two commonly used pruning criteria for neural networks, namely\nMagnitude based pruning Fisher information based pruning The motivation for this blogpost is to highlight that those pruning criteria are closely related while having different bounds on the resulting errors after pruning. I created this blogpost because I found this information lacking in many papers that used the concepts introduced below.\nIntroduction A common fully-connected neural network - as the name suggests - possesses “synapses”, i.e. weights, between all neurons of two adjacent layers. While those weights carry the “knowledge” of such a model, more weights does not always mean, higher “intelligence”. It was in fact already early recognized, that a fully-connected model is generally overparameterized and today, even “small” language models such as BERT function almost equally well with 90% of the weights removed. Such observations have been summarized by the Lottery Ticket Hypothesis saying,\nA trained fully-connected neural network contains a “much smaller” subnetwork that when reinitialized and trained exhibits at least the same classification performance.\nOr in simpler terms: It is not necessary to keep all synapses between all neurons after training.\nThese observations motivate weight pruning, i.e. “removing” certain weights between certain neurons with the hope of accelerating a model’s training and inference time.\nWeight Pruning Weight pruning is the process of\nIdentifying unimportant weights between neurons and, “Removing” them by setting their corresponding weight values to 0. But why? Pruning weights results in weight matrices becoming sparse, which, theoretically, require fewer multiply-accumulate-operations (i.e. computing time) when being part of a tensor product. Note that sparsity under loose constraints does not generally translate into less computation time on all hardware, which we do not further cover, here.\nAnd how? While step 2. is trivial, step 1. is not and is the reason for myriads of research papers in the field. In the following, we focus on this step by presenting two possible and common ways of identifying unimportant weights and motivating them mathematically.\nTaylor based saliency criteria While we used the term of a weight’s importance above, the correct jargon would be a weight’s saliency. We stick with importance for ease of reading.\nIn the following, we represent a neural network’s loss function by its Taylor approximation. If you are not familiar with it, you can still understand all of the presented weight importance measures and just skip the parts that use the Taylor approximation.\nLet a classifier neural network $f_{\\omega}: \\mathcal X \\rightarrow [0,1]^c$ be given, where $\\mathcal X$ denotes its input space (e.g. set of images) and $c$ the amount of classes to be classified (e.g. number of animals to be differentiated in the images). Let further $L$ denote the network’s loss function (we don’t formalize ground truth labels, here) and we further implicitly assume $\\mathcal X$ to be associated with a probability space with random variable $X$. Lastly, let $\\omega$ denote the network’s weights in a set of possible weights $\\Omega$. Then, the network’s typical training objective is\n$$ \\text{min}_{\\omega \\in \\Omega} \\space \\mathbb{E}_X \\left [ L\\left (f_{\\omega}(X)\\right )\\right ]. $$ Further, we will write $f$ instead of $f_{\\omega}$ for ease of notation.\nLet $\\omega_i$ be some weight whose importance / saliency we seek to estimate. Further, in the converged / trained network, let $\\omega_i = w$. Then, we can approximate the model’s loss function $L$ for $\\omega_i=0$ by:\n$$L(f(X) \\space| \\space\\omega_i = 0) $$\n$$ = L(f(X) \\space|\\space \\omega_i = w) + \\mathcal O(|w|)$$\n$$ = L(f(X) \\space|\\space \\omega_i = w) - w \\space \\frac{\\partial}{\\partial \\omega_i}L(f(X) \\space|\\space \\omega_i = w) + \\mathcal O(|w|^2)$$\n$$= L(f(X) \\space|\\space \\omega_i = w) - w \\space \\frac{\\partial}{\\partial \\omega_i}L(f(X) \\space|\\space \\omega_i = w) + \\frac{w^2}{2} \\space \\frac{\\partial^2}{\\partial \\omega_i^2}L(f(X) \\space|\\space \\omega_i = w) + \\mathcal O(|w|^3).$$\nOr as we want to minimize the change in loss after pruning a certain weight, we write\n$$ E := \\mathbb{E}_X\\left [ L(f(X) \\space|\\space \\omega_i = 0) - L(f(X) \\space| \\space\\omega_i = w) \\right ] $$\n$$ = \\mathcal{O(|w|)}$$\n$$ = -w \\space \\mathbb{E}_X \\left [ \\space \\frac{\\partial}{\\partial \\omega_i}L(f(X) \\space|\\space \\omega_i = w) \\right ] + \\mathcal O(|w|^2) $$\n$$ = -w \\space \\mathbb{E}_X \\left [ \\space \\frac{\\partial}{\\partial \\omega_i}L(f(X) \\space|\\space \\omega_i = w) \\right ] + \\frac{w^2}{2}\\space \\mathbb{E}_X \\left [ \\frac{\\partial^2}{\\partial \\omega_i^2}L(f(X) \\space|\\space \\omega_i = w) \\right ] + \\mathcal O(|w|^3)$$\nBut why? The above Taylor approximation enables us to approximate the model’s loss with $\\omega_i$ pruned without actually having to compute the loss in the pruned network. But don’t we have to compute other terms instead, then? Yes - but they are almost trivial to compute as we will show next.\nMaginitude based pruning The idea of magnitude based pruning lies in smaller weights (in absolute value) are less important.\nThis criterion is very simple but surprisingly effective. E.g. as the authors of the Lottery Ticket Hypothesis have shown, this criterion is effective enough to prune about 90% of weights in common architectures (iteratively).\nThe intuition behind why it effectively serves as an importance measure is that we would typically expect small changes in a certain weight to lead to small changes in the model’s classification performance. Setting weights to zero which are already close to zero may thus bear little impact on the model’s loss.\nMore formally, the Taylor approximation of\n$$ E = \\mathbb{E}_X\\left [ L(f(X) \\space|\\space \\omega_i = w) - L(f(X) \\space| \\space\\omega_i = 0) \\right ] = \\mathcal{O}(|w|)$$\nbounds the error $E$ by $\\mathcal O(|w|)$ which is exactly the motivation for this criterion.\nIn fact, the error can even be bounded more tightly as\n$$E = -w \\space \\mathbb{E}_X \\left [ \\space \\frac{\\partial}{\\partial \\omega_i}L(f(X) \\space|\\space \\omega_i = w) \\right ] + \\mathcal O(|w|^2),$$\nwhose term $\\mathbb{E}_X \\left [ \\space \\frac{\\partial}{\\partial \\omega_i}L(f(X) \\space|\\space \\omega_i = w) \\right ]$ equals $0$ if the model has fully converged.\nFisher-information based pruning The Fisher-information is a common concept from statistics. Informally, it quantifies how strongly we expect the likelihood function $p$ of a model with parameter $\\theta$ to vary as the model’s observations $X$ vary according to the distribution with parameter value $\\theta = \\hat{\\theta}$ for some fixed $\\hat{\\theta}$. In other words: how much information do different observed values of $X$ carry about the unknown parameter $\\theta$ if its true value were $\\hat{\\theta}$. Formally, the Fisher-information $\\mathcal I(\\hat{\\theta})$ is given by:\n$$ \\mathcal I(\\hat{\\theta}) = \\mathbb{E}_{X | \\theta = \\hat{\\theta}} \\left [ \\left ( \\frac{\\partial}{\\partial \\theta} \\log p(X | \\theta) \\space | \\space {\\theta=\\hat{\\theta}}\\right )^2\\right ].$$ The key-takeaway is that the above formulation only relies on first derivatives.\nIn the context of our model $f$ (which models a density/likelihood function), the parameter $\\theta$ is $\\omega_i$, $\\hat{\\theta} = w$, and $X$ is our input data. Then, the Fisher-information of $w_i$ captures how much information the input data carries about $\\omega_i$, or how important $\\omega_i$ is for the model’s classification performance. It therefore seems promising to use the Fisher-information as a proxy for a weight’s importance, right? Almost! With some slight modifications…\nOne fascinating property of the Fisher-information is that (under some regularity conditions), it can also be expressed using the second derivative by:\n$$ \\mathcal I(\\hat{\\theta}) = \\mathbb{E}_{X | \\theta = \\hat{\\theta}} \\left [ \\frac{\\partial^2}{\\partial \\theta^2} \\log p(X | \\theta) \\space | \\space {\\theta=\\hat{\\theta}}\\right ]. $$ The two presented formulations connect the first and second derivative of the the log-likelihood $\\log p$ and show that they even equal in expectation. This powerful property enables us to save tremendous computational effort as computing second derivatives is computationally very expensive.\nWe can make us of this advantage in classifiers that have been trained using the cross-entropy loss: the cross-entropy loss in combination with one-hot targets “collapses” to the logarithm of the predicted probability for the correct class label. Therefore,\n$$ \\mathbb{E}_X \\left [ \\frac{\\partial^2}{\\partial \\omega_i^2}L(f(X) \\space|\\space \\omega_i = w) \\right ] $$\nis just the Fisher information of the distribution that the model models for the correct class of each input. Therefore, we rewrite\n$$E = -w \\space \\mathbb{E}_X \\left [ \\space \\frac{\\partial}{\\partial \\omega_i}L(f(X) \\space|\\space \\omega_i = w) \\right ] + \\frac{w^2}{2}\\space \\mathbb{E}_X \\left [ \\frac{\\partial^2}{\\partial \\omega_i^2}L(f(X) \\space|\\space \\omega_i = w) \\right ] + \\mathcal O(|w|^3)$$\n$$ = -w \\space \\mathbb{E}_X \\left [ \\space \\frac{\\partial}{\\partial \\omega_i}L(f(X) \\space|\\space \\omega_i = w) \\right ] + \\frac{w^2}{2}\\space \\mathbb{E}_X \\left [ \\left ( \\frac{\\partial}{\\partial \\omega_i}L(f(X) \\space|\\space \\omega_i = w) \\right )^2 \\right ] + \\mathcal O(|w|^3),$$\nand got rid of second derivatives.\nIf we assume that the model has been trained to convergence, i.e. that the loss is at a local minimum in expectation, the above further reduces to\n$$ = \\frac{w^2}{2}\\space \\mathbb{E}_X \\left [ \\left ( \\frac{\\partial}{\\partial \\omega_i}L(f(X) \\space|\\space \\omega_i = w) \\right )^2 \\right ] + \\mathcal O(|w|^3),$$\nbecause the first term is zero at the local minimum.\nWhat have we gained? Again, we can approximate the weight’s importance score by only relying on already computed gradients. The expectation would simply be replaced by the empirical mean over a sufficiently large data-batch.\nComparing the above criteria Similarities Both of the above saliency criteria can be motivated by minimizing the resulting pruning error from the Taylor-expansion point of view. They also have in common that they require very little compute resources assuming precomputed gradients (which is given in realistic scenarios). They are both easy and quick to implement.\nDifferences The difference between the above criteria lies in the order at which they approximate the resulting pruning error. While magnitude based pruning is guaranteed to quantify the error at $\\mathcal O(|w|)$ (and even $\\mathcal O(|w|^2)$ for converged models), Fisher-information pruning bounds the error even tighter at $\\mathcal{O}(|w|^3)$.\nIn practice I hope that I can soon, if time permits, run a comparison between both approaches on a common model. I will add that here.\n",
  "wordCount" : "1613",
  "inLanguage": "en",
  "datePublished": "2024-07-22T00:00:00Z",
  "dateModified": "2024-07-22T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Frederic Mrozinski"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://fredericmrozinski.github.io/built-blog/posts/neuron-pruning-1/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Freddy's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://fredericmrozinski.github.io/built-blog/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://fredericmrozinski.github.io/built-blog/" accesskey="h" title="Freddy&#39;s Blog (Alt + H)">Freddy&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Taylor-Series based pruning criteria for weight pruning in neural networks
    </h1>
    <div class="post-meta"><span title='2024-07-22 00:00:00 +0000 UTC'>July 22, 2024</span>&nbsp;·&nbsp;Frederic Mrozinski

</div>
  </header> 
  <div class="post-content"><h1 id="summary">Summary<a hidden class="anchor" aria-hidden="true" href="#summary">#</a></h1>
<p>This article discusses how the Taylor series motivates two commonly used pruning criteria for neural networks, namely</p>
<ul>
<li>Magnitude based pruning</li>
<li>Fisher information based pruning</li>
</ul>
<p>The motivation for this blogpost is to highlight that those pruning criteria are closely related while having different bounds on the resulting errors after pruning.
I created this blogpost because I found this information lacking in many papers that used the concepts introduced below.</p>
<h1 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h1>
<p>A common <em>fully</em>-connected neural network - as the name suggests - possesses &ldquo;synapses&rdquo;, i.e. weights, between <em>all</em> neurons of two adjacent layers.
While those weights carry the &ldquo;knowledge&rdquo; of such a model, more weights does not always mean, higher &ldquo;intelligence&rdquo;.
It was in fact already early recognized, that a fully-connected model is generally overparameterized and today, even &ldquo;small&rdquo; language models such as BERT function almost equally well with 90% of the weights removed.
Such observations have been summarized by the <em>Lottery Ticket Hypothesis</em> saying,</p>
<blockquote>
<p><em>A trained fully-connected neural network contains a &ldquo;much smaller&rdquo; subnetwork that when reinitialized and trained exhibits at least the same classification performance.</em></p>
</blockquote>
<p>Or in simpler terms: It is not necessary to keep all synapses between all neurons after training.</p>
<p>These observations motivate <em>weight pruning</em>, i.e. &ldquo;removing&rdquo; certain weights between certain neurons with the hope of accelerating a model&rsquo;s training and inference time.</p>
<h1 id="weight-pruning">Weight Pruning<a hidden class="anchor" aria-hidden="true" href="#weight-pruning">#</a></h1>
<p>Weight pruning is the process of</p>
<ol>
<li>Identifying unimportant weights between neurons and,</li>
<li>&ldquo;Removing&rdquo; them by setting their corresponding weight values to 0.</li>
</ol>
<p>But why? Pruning weights results in weight matrices becoming sparse, which, theoretically,
require fewer multiply-accumulate-operations (i.e. computing time) when being part of a tensor product.
Note that sparsity under loose constraints does not generally translate into less computation time on all hardware, which we do not further cover, here.</p>
<p>And how? While step 2. is trivial, step 1. is not and is the reason for myriads of research papers in the field. In the following, we focus on this step by
presenting two possible and common ways of identifying unimportant weights and motivating them mathematically.</p>
<h1 id="taylor-based-saliency-criteria">Taylor based saliency criteria<a hidden class="anchor" aria-hidden="true" href="#taylor-based-saliency-criteria">#</a></h1>
<p>While we used the term of a weight&rsquo;s <em>importance</em> above, the correct jargon would be a weight&rsquo;s <em>saliency</em>.
We stick with <em>importance</em> for ease of reading.</p>
<p>In the following, we represent a neural network&rsquo;s loss function by its Taylor approximation. If you are not familiar with it, you can still understand all of the presented weight importance measures and just skip the parts that use the Taylor approximation.</p>
<p>Let a classifier neural network $f_{\omega}: \mathcal X \rightarrow [0,1]^c$ be given, where $\mathcal X$ denotes its input space (e.g. set of images) and $c$ the amount of classes to be classified (e.g. number of animals to be differentiated in the images). Let further $L$ denote the network&rsquo;s loss function (we don&rsquo;t formalize ground truth labels, here) and we further implicitly assume $\mathcal X$ to be associated with a probability space with random variable $X$.
Lastly, let $\omega$ denote the network&rsquo;s weights in a set of possible weights $\Omega$. Then, the network&rsquo;s typical training objective is</p>

$$ \text{min}_{\omega \in \Omega} \space \mathbb{E}_X \left [ L\left (f_{\omega}(X)\right )\right ]. $$

<p>Further, we will write $f$ instead of $f_{\omega}$ for ease of notation.</p>
<p>Let $\omega_i$ be some weight whose importance / saliency we seek to estimate. Further, in the converged / trained network, let $\omega_i = w$. Then, we can approximate the model&rsquo;s loss function $L$ for $\omega_i=0$ by:</p>
<p>$$L(f(X) \space| \space\omega_i = 0) $$</p>
<p>$$ = L(f(X) \space|\space \omega_i = w) + \mathcal O(|w|)$$</p>
<p>$$ = L(f(X) \space|\space \omega_i = w) - w \space \frac{\partial}{\partial \omega_i}L(f(X) \space|\space \omega_i = w) + \mathcal O(|w|^2)$$</p>
<p>$$= L(f(X) \space|\space \omega_i = w) - w \space \frac{\partial}{\partial \omega_i}L(f(X) \space|\space \omega_i = w) + \frac{w^2}{2} \space \frac{\partial^2}{\partial \omega_i^2}L(f(X) \space|\space \omega_i = w) + \mathcal O(|w|^3).$$</p>
<p>Or as we want to minimize the change in loss after pruning a certain weight, we write</p>
<p>$$ E := \mathbb{E}_X\left [ L(f(X) \space|\space \omega_i = 0) - L(f(X) \space| \space\omega_i = w) \right ] $$</p>
<p>$$ = \mathcal{O(|w|)}$$</p>
<p>$$ = -w \space \mathbb{E}_X \left [ \space \frac{\partial}{\partial \omega_i}L(f(X) \space|\space \omega_i = w) \right ] + \mathcal O(|w|^2) $$</p>
<p>$$ = -w \space \mathbb{E}_X \left [ \space \frac{\partial}{\partial \omega_i}L(f(X) \space|\space \omega_i = w) \right ] + \frac{w^2}{2}\space \mathbb{E}_X \left [ \frac{\partial^2}{\partial \omega_i^2}L(f(X) \space|\space \omega_i = w) \right ] + \mathcal O(|w|^3)$$</p>
<p>But why? The above Taylor approximation enables us to approximate the model&rsquo;s loss with $\omega_i$ pruned <em>without</em> actually having to compute the loss in the pruned network.
But don&rsquo;t we have to compute other terms instead, then? Yes - but they are almost trivial to compute as we will show next.</p>
<h2 id="maginitude-based-pruning">Maginitude based pruning<a hidden class="anchor" aria-hidden="true" href="#maginitude-based-pruning">#</a></h2>
<p>The idea of magnitude based pruning lies in <em>smaller weights (in absolute value) are less important</em>.</p>
<p>This criterion is very simple but surprisingly effective. E.g. as the authors of the Lottery Ticket Hypothesis have shown, this criterion is effective enough to prune about 90% of weights in common architectures (iteratively).</p>
<p>The intuition behind why it effectively serves as an importance measure is that we would typically expect small changes in a certain weight to lead to small changes in the model&rsquo;s classification performance.
Setting weights to zero which are already close to zero may thus bear little impact on the model&rsquo;s loss.</p>
<p>More formally, the Taylor approximation of</p>
<p>$$ E = \mathbb{E}_X\left [ L(f(X) \space|\space \omega_i = w) - L(f(X) \space| \space\omega_i = 0) \right ] = \mathcal{O}(|w|)$$</p>
<p>bounds the error $E$ by $\mathcal O(|w|)$ which is exactly the motivation for this criterion.</p>
<p>In fact, the error can even be bounded more tightly as</p>
<p>$$E = -w \space \mathbb{E}_X \left [ \space \frac{\partial}{\partial \omega_i}L(f(X) \space|\space \omega_i = w) \right ] + \mathcal O(|w|^2),$$</p>
<p>whose term $\mathbb{E}_X \left [ \space \frac{\partial}{\partial \omega_i}L(f(X) \space|\space \omega_i = w) \right ]$ equals $0$ if the model has fully converged.</p>
<h2 id="fisher-information-based-pruning">Fisher-information based pruning<a hidden class="anchor" aria-hidden="true" href="#fisher-information-based-pruning">#</a></h2>
<p>The Fisher-information is a common concept from statistics. Informally, it quantifies how strongly we expect the likelihood function $p$ of a model with parameter $\theta$ to vary as the model&rsquo;s observations $X$ vary according to the distribution with parameter value $\theta = \hat{\theta}$ for some fixed $\hat{\theta}$.
In other words: how much information do different observed values of $X$ carry about the unknown parameter $\theta$ if its true value were $\hat{\theta}$. Formally, the Fisher-information $\mathcal I(\hat{\theta})$ is given by:</p>

$$ \mathcal I(\hat{\theta}) = \mathbb{E}_{X | \theta = \hat{\theta}} \left [ \left ( \frac{\partial}{\partial \theta} \log p(X | \theta) \space | \space {\theta=\hat{\theta}}\right )^2\right ].$$

<p>The key-takeaway is that the above formulation only relies on <em>first</em> derivatives.</p>
<p>In the context of our model $f$ (which models a density/likelihood function), the parameter $\theta$ is $\omega_i$, $\hat{\theta} = w$, and $X$ is our input data. Then, the Fisher-information of $w_i$ captures how much information the input data carries about $\omega_i$, or how important $\omega_i$ is for the model&rsquo;s classification performance.
It therefore seems promising to use the Fisher-information as a proxy for a weight&rsquo;s importance, right?
Almost! With some slight modifications&hellip;</p>
<p>One fascinating property of the Fisher-information is that (under some regularity conditions), it can also be expressed using the <em>second</em> derivative by:</p>

$$ \mathcal I(\hat{\theta}) = \mathbb{E}_{X | \theta = \hat{\theta}} \left [  \frac{\partial^2}{\partial \theta^2} \log p(X | \theta) \space | \space {\theta=\hat{\theta}}\right ]. $$

<p>The two presented formulations connect the <em>first</em> and <em>second</em> derivative of the the log-likelihood $\log p$ and show that they even <em>equal</em> in expectation.
This powerful property enables us to save tremendous computational effort as computing second derivatives is computationally <em>very</em> expensive.</p>
<p>We can make us of this advantage in classifiers that have been trained using the cross-entropy loss: the cross-entropy loss in combination with one-hot targets &ldquo;collapses&rdquo; to the logarithm of the predicted probability for the correct class label. Therefore,</p>
<p>$$ \mathbb{E}_X \left [ \frac{\partial^2}{\partial \omega_i^2}L(f(X) \space|\space \omega_i = w) \right ] $$</p>
<p>is just the Fisher information of the distribution that the model models for the correct class of each input. Therefore, we rewrite</p>
<p>$$E  = -w \space \mathbb{E}_X \left [ \space \frac{\partial}{\partial \omega_i}L(f(X) \space|\space \omega_i = w) \right ] + \frac{w^2}{2}\space \mathbb{E}_X \left [ \frac{\partial^2}{\partial \omega_i^2}L(f(X) \space|\space \omega_i = w) \right ] + \mathcal O(|w|^3)$$</p>
<p>$$ = -w \space \mathbb{E}_X \left [ \space \frac{\partial}{\partial \omega_i}L(f(X) \space|\space \omega_i = w) \right ] + \frac{w^2}{2}\space \mathbb{E}_X \left [ \left ( \frac{\partial}{\partial \omega_i}L(f(X) \space|\space \omega_i = w) \right )^2 \right ] + \mathcal O(|w|^3),$$</p>
<p>and got rid of second derivatives.</p>
<p>If we assume that the model has been trained to convergence, i.e. that the loss is at a local minimum in expectation, the above further reduces to</p>
<p>$$ = \frac{w^2}{2}\space \mathbb{E}_X \left [ \left ( \frac{\partial}{\partial \omega_i}L(f(X) \space|\space \omega_i = w) \right )^2 \right ] + \mathcal O(|w|^3),$$</p>
<p>because the first term is zero at the local minimum.</p>
<p>What have we gained? Again, we can approximate the weight&rsquo;s importance score by only relying on already computed gradients. The expectation would simply be replaced by the empirical mean over
a sufficiently large data-batch.</p>
<h1 id="comparing-the-above-criteria">Comparing the above criteria<a hidden class="anchor" aria-hidden="true" href="#comparing-the-above-criteria">#</a></h1>
<h3 id="similarities">Similarities<a hidden class="anchor" aria-hidden="true" href="#similarities">#</a></h3>
<p>Both of the above saliency criteria can be motivated by minimizing the resulting pruning error from the Taylor-expansion point of view. They also have in common that they require
very little compute resources assuming precomputed gradients (which is given in realistic scenarios). They are both easy and quick to implement.</p>
<h3 id="differences">Differences<a hidden class="anchor" aria-hidden="true" href="#differences">#</a></h3>
<p>The difference between the above criteria lies in the order at which they approximate the resulting pruning error. While magnitude based pruning is guaranteed to quantify the error at $\mathcal O(|w|)$ (and even $\mathcal O(|w|^2)$ for converged models), Fisher-information pruning bounds the error even tighter at $\mathcal{O}(|w|^3)$.</p>
<h3 id="in-practice">In practice<a hidden class="anchor" aria-hidden="true" href="#in-practice">#</a></h3>
<p>I hope that I can soon, if time permits, run a comparison between both approaches on a common model. I will add that here.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://fredericmrozinski.github.io/built-blog/">Freddy&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
