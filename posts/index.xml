<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Freddy&#39;s Blog</title>
    <link>https://fredericmrozinski.github.io/built-blog/posts/</link>
    <description>Recent content in Posts on Freddy&#39;s Blog</description>
    <generator>Hugo -- 0.131.0</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 Aug 2024 13:12:10 +0200</lastBuildDate>
    <atom:link href="https://fredericmrozinski.github.io/built-blog/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title> Jade Scientific Calculator - Past, Present, and Future</title>
      <link>https://fredericmrozinski.github.io/built-blog/posts/jsc/</link>
      <pubDate>Fri, 09 Aug 2024 13:12:10 +0200</pubDate>
      <guid>https://fredericmrozinski.github.io/built-blog/posts/jsc/</guid>
      <description>Jade Scientific Calculator - Past, Present, and Future Jade Scientific Calculator (JSC) is an Android application designed for advanced mathematical calculations, yet simple enough to be a daily tool for basic computations. From basic arithmetic to solving differential equations and linear algebra problems, this app has you covered.
Past - Eager Development and the Pandemic JSC began as my capstone project in high school. During my AP courses, I fell in love with my HP Prime calculator but grew frustrated that our phones, despite their phenomenal hardware, lacked an equally good and user-friendly app.</description>
    </item>
    <item>
      <title>Taylor-Series based pruning criteria for weight pruning in neural networks</title>
      <link>https://fredericmrozinski.github.io/built-blog/posts/neuron-pruning-1/</link>
      <pubDate>Mon, 22 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://fredericmrozinski.github.io/built-blog/posts/neuron-pruning-1/</guid>
      <description>TL;DR By comparing magnitude based pruning with Fisher-information based pruning, the theory provides a tighter loss error bound after pruning. In practice however, both methods yield the same performance, on average. Fisher-information pruning costs more compute time. From our observed examples, we suggest to use magnitude based pruning considering same performance in less time. Summary This article discusses how the Taylor series motivates two commonly used pruning criteria for neural networks, namely</description>
    </item>
  </channel>
</rss>
