<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Freddy&#39;s Blog</title>
    <link>https://fredericmrozinski.github.io/built-blog/</link>
    <description>Recent content on Freddy&#39;s Blog</description>
    <generator>Hugo -- 0.129.0</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 22 Jul 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://fredericmrozinski.github.io/built-blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Taylor-Series based pruning criteria for weight pruning in neural networks</title>
      <link>https://fredericmrozinski.github.io/built-blog/posts/neuron-pruning-1/</link>
      <pubDate>Mon, 22 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://fredericmrozinski.github.io/built-blog/posts/neuron-pruning-1/</guid>
      <description>Summary This article discusses how the Taylor series motivates two commonly used pruning criteria for neural networks, namely
Magnitude based pruning Fisher information based pruning The motivation for this blogpost is to highlight that those pruning criteria are closely related while having different bounds on the resulting errors after pruning. I created this blogpost because I found this information lacking in many papers that used the concepts introduced below.
Introduction A common fully-connected neural network - as the name suggests - possesses &amp;ldquo;synapses&amp;rdquo;, i.</description>
    </item>
  </channel>
</rss>
